{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-11T16:56:35.009098Z",
     "start_time": "2025-05-11T16:56:34.995513Z"
    }
   },
   "source": "import os",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:56:35.584768Z",
     "start_time": "2025-05-11T16:56:35.570719Z"
    }
   },
   "cell_type": "code",
   "source": "%pwd",
   "id": "b29584a66ffdd94f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\DIKSHANT PATEL\\\\Kidney-Disease-Classification\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:56:36.227777Z",
     "start_time": "2025-05-11T16:56:36.218777Z"
    }
   },
   "cell_type": "code",
   "source": "os.chdir(\"../.\")",
   "id": "5ff8ac6f33a3573d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:56:36.786333Z",
     "start_time": "2025-05-11T16:56:36.770334Z"
    }
   },
   "cell_type": "code",
   "source": "%pwd",
   "id": "68610b95d07c3f45",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\DIKSHANT PATEL\\\\Kidney-Disease-Classification'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T20:18:28.091491Z",
     "start_time": "2025-05-11T20:18:28.078489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DataLoaderConfig:\n",
    "    root_dir: str\n",
    "    train_data: str\n",
    "    valid_data: str\n",
    "    test_data: str\n",
    "    target_size: tuple\n",
    "    batch_size: int\n",
    "    color_mode: str\n",
    "    class_mode: str\n",
    "    seed: int\n",
    "    rotation_range: int\n",
    "    width_shift_range: float\n",
    "    height_shift_range: float\n",
    "    shear_range: float\n",
    "    zoom_range: float\n",
    "    horizontal_flip: bool\n",
    "    fill_mode: str"
   ],
   "id": "440b17550e6007f9",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T20:18:28.418037Z",
     "start_time": "2025-05-11T20:18:28.413033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from cnnClassifier.constants import *\n",
    "from cnnClassifier.utils.common import read_yaml, create_directories"
   ],
   "id": "1f529468020e87a8",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T20:18:28.745831Z",
     "start_time": "2025-05-11T20:18:28.737822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath = CONFIG_FILE_PATH, params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        \n",
    "    def get_data_loader_config(self) -> DataLoaderConfig:\n",
    "        config = self.config.data_loader\n",
    "        params = self.params.data_loader\n",
    "        return DataLoaderConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            train_data=config.train_data,\n",
    "            valid_data=config.valid_data,\n",
    "            test_data=config.test_data,\n",
    "            target_size=params.target_size,\n",
    "            batch_size=params.batch_size,\n",
    "            color_mode=params.color_mode,\n",
    "            class_mode=params.class_mode,\n",
    "            seed=params.seed,\n",
    "            rotation_range=params.rotation_range,\n",
    "            width_shift_range=params.width_shift_range,\n",
    "            height_shift_range=params.height_shift_range,\n",
    "            shear_range=params.shear_range,\n",
    "            zoom_range=params.zoom_range,\n",
    "            horizontal_flip=params.horizontal_flip,\n",
    "            fill_mode = params.fill_mode\n",
    "        )\n",
    "        "
   ],
   "id": "7f60c6dac3c6e5d1",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T20:18:29.185134Z",
     "start_time": "2025-05-11T20:18:29.175136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from cnnClassifier import logger"
   ],
   "id": "f5f9004cce89d18c",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T20:41:21.206870Z",
     "start_time": "2025-05-11T20:41:21.183908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DataLoader: \n",
    "    def __init__(self, config: DataLoaderConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def create_generator(self):\n",
    "        aug = ImageDataGenerator(\n",
    "            rescale=1.0 / 255,\n",
    "            rotation_range=self.config.rotation_range,\n",
    "            width_shift_range=self.config.width_shift_range,\n",
    "            height_shift_range=self.config.height_shift_range,\n",
    "            shear_range=self.config.shear_range,\n",
    "            zoom_range=self.config.zoom_range,\n",
    "            horizontal_flip=self.config.horizontal_flip,\n",
    "            fill_mode=self.config.fill_mode,\n",
    "        )\n",
    "        ori = ImageDataGenerator(rescale=1.0 / 255)\n",
    "        logger.info(\"Generators created successfully.\")\n",
    "        return aug, ori\n",
    "    \n",
    "    def load_dataframe(self, file_path):\n",
    "        logger.info(f\"Loading dataframe from file: {file_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            logger.info(f\"Dataframe loaded successfully with {len(df)} records.\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading dataframe from {file_path}: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "    def validate_filepaths(self, df):\n",
    "        # Check if the file paths are valid and log invalid paths\n",
    "        df['valid_filepath'] = df['filepath'].apply(lambda x: os.path.isfile(x))\n",
    "        invalid_files = df[df['valid_filepath'] == False]\n",
    "        \n",
    "        if not invalid_files.empty:\n",
    "            logger.warning(f\"Found {len(invalid_files)} invalid file paths!\")\n",
    "            logger.warning(f\"Invalid file paths: \\n{invalid_files[['filepath']]}\")\n",
    "        \n",
    "        logger.info(f\"Valid file paths count: {df['valid_filepath'].sum()}\")\n",
    "        # Return only rows with valid file paths\n",
    "        return df[df['valid_filepath']]\n",
    "\n",
    "    def get_flow(self, df, generator, shuffle=True):\n",
    "        return generator.flow_from_dataframe(\n",
    "            dataframe=df,\n",
    "            x_col=\"filepath\",        \n",
    "            y_col=\"label\",            \n",
    "            target_size=(224,224),\n",
    "            batch_size=self.config.batch_size,  \n",
    "            class_mode=self.config.class_mode, \n",
    "            color_mode=self.config.color_mode,  \n",
    "            shuffle=shuffle,  \n",
    "            seed=self.config.seed  \n",
    "        )\n",
    "\n",
    "    def combined_generator(self, aug, ori):\n",
    "        logger.info(f\"Combining augmented and original data generators.\")\n",
    "        batch_size = self.config.batch_size\n",
    "        n_orig = batch_size // 2\n",
    "        n_aug = batch_size - n_orig\n",
    "        logger.info(f\"Type of aug: {type(aug)}\")\n",
    "        while True:\n",
    "            aug_images, aug_labels = next(aug)\n",
    "            ori_images, ori_labels = next(ori)\n",
    "            \n",
    "            logger.info(f\"Augmented image batch shape: {aug_images.shape}\")\n",
    "            logger.info(f\"Augmented labels shape: {aug_labels.shape}\")\n",
    "            logger.info(f\"Original image batch shape: {ori_images.shape}\")\n",
    "            logger.info(f\"Original labels shape: {ori_labels.shape}\")\n",
    "            \n",
    "            # Combine the two datasets\n",
    "            images = np.concatenate((ori_images[:n_orig], aug_images[n_aug:]), axis=0)\n",
    "            labels = np.concatenate((ori_labels[:n_orig], aug_labels[n_aug:]), axis=0)\n",
    "\n",
    "            yield images, labels\n",
    "    \n",
    "    def get_generators(self):\n",
    "        # File paths for the CSVs\n",
    "        train_path = f\"{self.config.root_dir}/{self.config.train_data}\"\n",
    "        valid_path = f\"{self.config.root_dir}/{self.config.valid_data}\"\n",
    "        test_path = f\"{self.config.root_dir}/{self.config.test_data}\"\n",
    "\n",
    "        # Load the dataframes\n",
    "        try:\n",
    "            train_df = self.load_dataframe(train_path)\n",
    "            valid_df = self.load_dataframe(valid_path)\n",
    "            test_df = self.load_dataframe(test_path)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading dataframes: {str(e)}\")\n",
    "            raise e\n",
    "        \n",
    "        # Validate file paths\n",
    "        train_df = self.validate_filepaths(train_df)\n",
    "        valid_df = self.validate_filepaths(valid_df)\n",
    "        test_df = self.validate_filepaths(test_df)\n",
    "        \n",
    "        # Ensure the labels are in the correct format (str)\n",
    "        train_df['label'] = train_df['label'].astype(str)\n",
    "        valid_df['label'] = valid_df['label'].astype(str)\n",
    "        test_df['label'] = test_df['label'].astype(str)\n",
    "        \n",
    "        # Create generators\n",
    "        aug_gen, ori_gen = self.create_generator()\n",
    "\n",
    "        # Get augmented and original image generators\n",
    "        aug_train = self.get_flow(train_df, aug_gen, shuffle=True)\n",
    "        ori_train = self.get_flow(train_df, ori_gen, shuffle=True)\n",
    "\n",
    "        # Combine the generators\n",
    "        train = self.combined_generator(aug_train, ori_train)\n",
    "        valid = self.get_flow(valid_df, ori_gen, shuffle=False)\n",
    "        test = self.get_flow(test_df, ori_gen, shuffle=False)\n",
    "        \n",
    "        logger.info(f\"Successfully created train, validation, and test generators.\")\n",
    "        return train, valid, test,train_df, ori_train\n"
   ],
   "id": "ae5edaa0a075f37d",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T20:18:32.885901Z",
     "start_time": "2025-05-11T20:18:30.483054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_loader_config = config.get_data_loader_config()\n",
    "    data_loader = DataLoader(config=data_loader_config)\n",
    "    train_generator, valid_generator, test_generator,train_df, ori_train = data_loader.get_generators()\n",
    "\n",
    "    # Trigger one batch to activate logging from combined_generator\n",
    "    logger.info(\"Fetching one batch to trigger logging inside combined_generator...\")\n",
    "    images, labels = next(train_generator)\n",
    "    logger.info(f\"Combined batch shape: {images.shape}, Labels shape: {labels.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Exception occurred during generator setup: {str(e)}\")\n",
    "    raise e"
   ],
   "id": "9580346885e19849",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-11 22:18:30,495: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-05-11 22:18:30,513: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-05-11 22:18:30,516: INFO: 144888782: Loading dataframe from file: artifacts/data_split/train.csv]\n",
      "[2025-05-11 22:18:30,545: INFO: 144888782: Dataframe loaded successfully with 8712 records.]\n",
      "[2025-05-11 22:18:30,546: INFO: 144888782: Loading dataframe from file: artifacts/data_split/val.csv]\n",
      "[2025-05-11 22:18:30,553: INFO: 144888782: Dataframe loaded successfully with 1121 records.]\n",
      "[2025-05-11 22:18:30,554: INFO: 144888782: Loading dataframe from file: artifacts/data_split/test.csv]\n",
      "[2025-05-11 22:18:30,566: INFO: 144888782: Dataframe loaded successfully with 2613 records.]\n",
      "[2025-05-11 22:18:31,215: INFO: 144888782: Valid file paths count: 8712]\n",
      "[2025-05-11 22:18:31,288: INFO: 144888782: Valid file paths count: 1121]\n",
      "[2025-05-11 22:18:31,428: INFO: 144888782: Valid file paths count: 2613]\n",
      "[2025-05-11 22:18:31,431: INFO: 144888782: Generators created successfully.]\n",
      "Found 8712 validated image filenames belonging to 4 classes.\n",
      "Found 8712 validated image filenames belonging to 4 classes.\n",
      "Found 1121 validated image filenames belonging to 4 classes.\n",
      "Found 2613 validated image filenames belonging to 4 classes.\n",
      "[2025-05-11 22:18:32,584: INFO: 144888782: Successfully created train, validation, and test generators.]\n",
      "[2025-05-11 22:18:32,586: INFO: 531340721: Fetching one batch to trigger logging inside combined_generator...]\n",
      "[2025-05-11 22:18:32,587: INFO: 144888782: Combining augmented and original data generators.]\n",
      "[2025-05-11 22:18:32,588: INFO: 144888782: Type of aug: <class 'keras.preprocessing.image.DataFrameIterator'>]\n",
      "[2025-05-11 22:18:32,868: INFO: 144888782: Augmented image batch shape: (32, 224, 224, 1)]\n",
      "[2025-05-11 22:18:32,869: INFO: 144888782: Augmented labels shape: (32, 4)]\n",
      "[2025-05-11 22:18:32,870: INFO: 144888782: Original image batch shape: (32, 224, 224, 1)]\n",
      "[2025-05-11 22:18:32,871: INFO: 144888782: Original labels shape: (32, 4)]\n",
      "[2025-05-11 22:18:32,874: INFO: 531340721: Combined batch shape: (32, 224, 224, 1), Labels shape: (32, 4)]\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "89bd61aa96befeb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "20248cce5753a4fa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
